{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['AO'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m transform_steps \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mImputeNumeric\u001b[39m\u001b[39m\"\u001b[39m, ImputeNumeric()),\n\u001b[1;32m     39\u001b[0m                    (\u001b[39m'\u001b[39m\u001b[39mRemoveSkewnessKurtosis\u001b[39m\u001b[39m'\u001b[39m, RemoveSkewnessKurtosis()),\n\u001b[1;32m     40\u001b[0m                    (\u001b[39m'\u001b[39m\u001b[39mStandardizeStandardScaler\u001b[39m\u001b[39m'\u001b[39m, Standardize(RobustScaler()))]\n\u001b[1;32m     41\u001b[0m transform_pipeline \u001b[39m=\u001b[39m Pipeline(transform_steps)\n\u001b[0;32m---> 43\u001b[0m X \u001b[39m=\u001b[39m transform_pipeline\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[0;32m/home/hassan/.conda/envs/mla/lib/python3.10/site-packages/sklearn/pipeline.py:658\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    656\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    657\u001b[0m \u001b[39mfor\u001b[39;00m _, _, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[0;32m--> 658\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    659\u001b[0m \u001b[39mreturn\u001b[39;00m Xt\n",
      "File \u001b[0;32m/home/hassan/.conda/envs/mla/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/FDA/src/models/transformation.py:118\u001b[0m, in \u001b[0;36mRemoveSkewnessKurtosis.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m--> 118\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_log_col(X)\n",
      "File \u001b[0;32m~/FDA/src/models/transformation.py:111\u001b[0m, in \u001b[0;36mRemoveSkewnessKurtosis.extract_log_col\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    109\u001b[0m cols_no_transform \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(statusdf[statusdf[\u001b[39m'\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNo\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mnumeric_col\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    110\u001b[0m log_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mInternalpatientid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m log_numeric_cols \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_cols \u001b[39m+\u001b[39m cols_no_transform \n\u001b[0;32m--> 111\u001b[0m X \u001b[39m=\u001b[39m X[log_cols]\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m/home/hassan/.conda/envs/mla/lib/python3.10/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m/home/hassan/.conda/envs/mla/lib/python3.10/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/home/hassan/.conda/envs/mla/lib/python3.10/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['AO'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import argparse\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "from sklearn.pipeline import  Pipeline, make_pipeline\n",
    "\n",
    "from transformation import RemoveSkewnessKurtosis, Standardize, ImputeNumeric\n",
    "\n",
    "\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "# Import Data\n",
    "path = '/home/daisy/FDA_Dataset/inpatient_all_final_1.csv'\n",
    "data = pd.read_csv(path).iloc[:,1:]\n",
    "X = data.drop(columns = ['CVD_readmission', 'readmission within 300 days'])\n",
    "y = column_or_1d(data[['readmission within 300 days']])\n",
    " \n",
    "\n",
    "\n",
    "# Split Train and Test\n",
    "transform_steps = [(\"ImputeNumeric\", ImputeNumeric()),\n",
    "                   ('RemoveSkewnessKurtosis', RemoveSkewnessKurtosis()),\n",
    "                   ('StandardizeStandardScaler', Standardize(RobustScaler()))]\n",
    "transform_pipeline = Pipeline(transform_steps)\n",
    "\n",
    "X = transform_pipeline.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import copy,os,sys,psutil\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from grid_search_cv import *\n",
    "\n",
    "gsearch = LGBM_Grid_CV(X,y, LGBM_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=pd.DataFrame(X['Internalpatientid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2['new'] =[0]*84536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Internalpatientid', 'num_stays', 'stay_length', 'num_unique_units',\n",
       "       'num_transfers', 'num_cvd_admission', 'CVD',\n",
       "       'unique_admitting_specialty', 'unique_discharging_specialty',\n",
       "       'DOMICILIARY', 'MEDICINE', 'NHCU', 'NON-COUNT', 'OTHERS', 'PSYCHIATRY',\n",
       "       'SURGERY', 'Age 20-40', 'Age 40-60', 'Age 60-80', 'Age 80-100',\n",
       "       'Age 100-120', 'age_mean', 'age_std', 'age_min', 'age_max', 'stay_min',\n",
       "       'stay_max', 'stay_mean', 'stay_std', 'freq', 'Medical', 'Mental',\n",
       "       'Others_Specialty', 'Rehab', 'Gerontology', 'CVD_readmission',\n",
       "       'readmission within 300 days', 'total_procedure', 'num_surgery_pro',\n",
       "       'Ethnicity', 'Gender', 'Races', 'Ethnicity_0', 'Ethnicity_1',\n",
       "       'Ethnicity_2', 'Races_0', 'Races_1', 'Races_2', 'Races_3',\n",
       "       'num_immunization', 'Num med per admission mean',\n",
       "       'Num med per admission min', 'Num med per admission max',\n",
       "       'Total medications', 'mean age at specailty', 'specialty count',\n",
       "       'period mean', 'period std', 'specialty medical count',\n",
       "       'specialty support count', 'Ruca category encoded',\n",
       "       'Age 20-40 hypotension', 'Age 40-60 hypotension',\n",
       "       'Age 60-80 hypotension', 'Age 80-100 hypotension',\n",
       "       'Age 100-120 hypotension', 'Age 20-40 healthy', 'Age 40-60 healthy',\n",
       "       'Age 60-80 healthy', 'Age 80-100 healthy', 'Age 100-120 healthy',\n",
       "       'Age 20-40 hypertension', 'Age 40-60 hypertension',\n",
       "       'Age 60-80 hypertension', 'Age 80-100 hypertension',\n",
       "       'Age 100-120 hypertension', 'lab_count', 'lab_freq', 'lab_age_mean',\n",
       "       'lab_age_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_samples': 10, 'n_estimators': 100, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "print(gsearch.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
